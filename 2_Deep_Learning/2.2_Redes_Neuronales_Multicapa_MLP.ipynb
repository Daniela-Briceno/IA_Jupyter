{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a46cd08c",
   "metadata": {},
   "source": [
    "# Redes Neuronales Multicapa (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16b7671",
   "metadata": {},
   "source": [
    "Una red MLP consiste en:\n",
    "- Una capa de entrada\n",
    "- Una o más capas ocultas con activaciones no lineales\n",
    "- Una capa de salida con softmax o activación lineal según el caso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ff3cee7-db20-44e3-bb8b-199b04905438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos cargados y procesados correctamente.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Preprocesamiento del dataset original\n",
    "df = pd.read_csv('Historia_Climatica.csv')\n",
    "df[\"Fecha_Hora\"] = pd.to_datetime(df[\"Fecha_Hora\"], format=\"%m/%d/%y %H:%M\")\n",
    "df[\"Presion_hPa\"] = df[\"Presion_hPa\"] / 10\n",
    "\n",
    "# Convertir clases a números\n",
    "le = LabelEncoder()\n",
    "df[\"Estado_Clima_Cod\"] = le.fit_transform(df[\"Estado_Clima\"])\n",
    "\n",
    "# Selección de características y etiquetas\n",
    "X = df[[\"Temperatura_C\", \"Humedad_%\", \"Vel_Viento_mps\", \"Presion_hPa\"]].values\n",
    "y = df[\"Estado_Clima_Cod\"].values\n",
    "\n",
    "# Escalado\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# División de datos\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "print(\"Datos cargados y procesados correctamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca638698",
   "metadata": {},
   "source": [
    "## Capas ocultas y neuronas (1 capa oculta, 16 neuronas ocultas)\n",
    "\n",
    "El número de capas ocultas y neuronas determina la **capacidad de representación** del modelo. Más capas → mayor complejidad, pero mayor riesgo de sobreajuste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "380a32c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "  return np.maximum(0, x)\n",
    "\n",
    "def softmax(x):\n",
    "  e_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "  return e_x / np.sum(e_x, axis=1, keepdims=True)\n",
    "\n",
    "epochs = 200\n",
    "\n",
    "class MLP:\n",
    "  def __init__(self, input_dim, hidden_dim, output_dim, lr=0.01):\n",
    "    self.W1 = np.random.randn(input_dim, hidden_dim) * np.sqrt(2. / input_dim)\n",
    "    self.b1 = np.zeros((1, hidden_dim))\n",
    "    self.W2 = np.random.randn(hidden_dim, output_dim) * np.sqrt(2. / hidden_dim)\n",
    "    self.b2 = np.zeros((1, output_dim))\n",
    "    self.lr = lr\n",
    "\n",
    "  def forward(self, X):\n",
    "    self.z1 = np.dot(X, self.W1) + self.b1\n",
    "    self.a1 = relu(self.z1)\n",
    "    self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "    self.a2 = softmax(self.z2)\n",
    "    return self.a2\n",
    "\n",
    "  def backward(self, X, y_true):\n",
    "    m = y_true.shape[0]\n",
    "    grad_output = self.a2.copy()\n",
    "    grad_output[range(m), y_true] -= 1\n",
    "    grad_output /= m\n",
    "\n",
    "    dW2 = np.dot(self.a1.T, grad_output)\n",
    "    db2 = np.sum(grad_output, axis=0, keepdims=True)\n",
    "\n",
    "    da1 = np.dot(grad_output, self.W2.T)\n",
    "    dz1 = da1 * (self.z1 > 0)\n",
    "\n",
    "    dW1 = np.dot(X.T, dz1)\n",
    "    db1 = np.sum(dz1, axis=0, keepdims=True)\n",
    "\n",
    "    self.W2 -= self.lr * dW2\n",
    "    self.b2 -= self.lr * db2\n",
    "    self.W1 -= self.lr * dW1\n",
    "    self.b1 -= self.lr * db1\n",
    "  \n",
    "  def train(self, X, y, epochs):\n",
    "    for epoch in range(epochs):\n",
    "      y_pred = self.forward(X)\n",
    "      loss = -np.sum(np.log(y_pred[range(len(y)), y])) / len(y)\n",
    "      acc = np.mean(np.argmax(y_pred, axis=1) == y)\n",
    "      self.backward(X, y)\n",
    "      if epoch % 10 == 0:\n",
    "        print(f\"Época {epoch}: Pérdida={loss:.4f}, Precisión={acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f6ff3f12-8828-45bb-b34c-d2309254132d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Época 0: Pérdida=2.4600, Precisión=0.1638\n",
      "Época 10: Pérdida=2.1085, Precisión=0.1799\n",
      "Época 20: Pérdida=1.9695, Precisión=0.1774\n",
      "Época 30: Pérdida=1.9014, Precisión=0.1811\n",
      "Época 40: Pérdida=1.8646, Precisión=0.1811\n",
      "Época 50: Pérdida=1.8425, Precisión=0.1762\n",
      "Época 60: Pérdida=1.8280, Precisión=0.1873\n",
      "Época 70: Pérdida=1.8175, Precisión=0.1886\n",
      "Época 80: Pérdida=1.8097, Precisión=0.1935\n",
      "Época 90: Pérdida=1.8034, Precisión=0.1898\n",
      "Época 100: Pérdida=1.7983, Precisión=0.1911\n",
      "Época 110: Pérdida=1.7939, Precisión=0.2010\n",
      "Época 120: Pérdida=1.7901, Precisión=0.1985\n",
      "Época 130: Pérdida=1.7869, Precisión=0.2047\n",
      "Época 140: Pérdida=1.7842, Precisión=0.2060\n",
      "Época 150: Pérdida=1.7818, Precisión=0.2047\n",
      "Época 160: Pérdida=1.7796, Precisión=0.2097\n",
      "Época 170: Pérdida=1.7777, Precisión=0.2109\n",
      "Época 180: Pérdida=1.7759, Precisión=0.2097\n",
      "Época 190: Pérdida=1.7743, Precisión=0.2159\n",
      "Precisión final en test: 0.1535\n"
     ]
    }
   ],
   "source": [
    "# Entrenar MLP\n",
    "mlp = MLP(input_dim=4, hidden_dim=16, output_dim=len(np.unique(y)), lr=0.1)\n",
    "mlp.train(X_train, y_train, epochs)\n",
    "\n",
    "# Evaluación\n",
    "y_pred_test = mlp.forward(X_test)\n",
    "test_accuracy = np.mean(np.argmax(y_pred_test, axis=1) == y_test)\n",
    "print(f\"Precisión final en test: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf97f8f-8216-4f4e-a324-c67ab90f090d",
   "metadata": {},
   "source": [
    "## Versión extendida del MLP con múltiples capas ocultas (flexible y escalable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0d1bd154-8ea1-42d4-aa00-38fe764e1e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "# Funciones de activación y sus derivadas\n",
    "def relu(x):\n",
    "  return np.maximum(0, x)\n",
    "\n",
    "def relu_derivada(x):\n",
    "  return (x > 0).astype(float)\n",
    "\n",
    "def softmax(x):\n",
    "  e_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "  return e_x / np.sum(e_x, axis=1, keepdims=True)\n",
    "\n",
    "class MLP_Flexible:\n",
    "  def __init__(self, capas: List[int], lr=0.01):\n",
    "    \"\"\"\n",
    "    capas: lista con el número de neuronas por capa (incluyendo entrada y salida)\n",
    "      Ej: [4, 32, 16, 6] → 2 capas ocultas\n",
    "    \"\"\"\n",
    "    self.capas = capas\n",
    "    self.lr = lr\n",
    "    self.W = []\n",
    "    self.b = []\n",
    "\n",
    "    for i in range(len(capas) - 1):\n",
    "      W_i = np.random.randn(capas[i], capas[i+1]) * np.sqrt(2 / capas[i])\n",
    "      b_i = np.zeros((1, capas[i+1]))\n",
    "      self.W.append(W_i)\n",
    "      self.b.append(b_i)\n",
    "\n",
    "  def forward(self, X):\n",
    "    self.a = [X]\n",
    "    self.z = []\n",
    "\n",
    "    for i in range(len(self.W) - 1):\n",
    "      z_i = np.dot(self.a[-1], self.W[i]) + self.b[i]\n",
    "      a_i = relu(z_i)\n",
    "      self.z.append(z_i)\n",
    "      self.a.append(a_i)\n",
    "\n",
    "    # Última capa (softmax)\n",
    "    z_final = np.dot(self.a[-1], self.W[-1]) + self.b[-1]\n",
    "    a_final = softmax(z_final)\n",
    "    self.z.append(z_final)\n",
    "    self.a.append(a_final)\n",
    "    return a_final\n",
    "\n",
    "  def backward(self, y_true):\n",
    "    m = y_true.shape[0]\n",
    "    grad = self.a[-1].copy()\n",
    "    grad[range(m), y_true] -= 1\n",
    "    grad /= m\n",
    "\n",
    "    for i in reversed(range(len(self.W))):\n",
    "      dW = np.dot(self.a[i].T, grad)\n",
    "      db = np.sum(grad, axis=0, keepdims=True)\n",
    "\n",
    "      self.W[i] -= self.lr * dW\n",
    "      self.b[i] -= self.lr * db\n",
    "\n",
    "      if i != 0:\n",
    "        grad = np.dot(grad, self.W[i].T) * relu_derivada(self.z[i-1])\n",
    "\n",
    "  def train(self, X, y, epochs):\n",
    "    for epoch in range(epochs):\n",
    "      y_pred = self.forward(X)\n",
    "      loss = -np.sum(np.log(y_pred[range(len(y)), y])) / len(y)\n",
    "      acc = np.mean(np.argmax(y_pred, axis=1) == y)\n",
    "      self.backward(y)\n",
    "      if epoch % 10 == 0:\n",
    "        print(f\"Época {epoch}: Pérdida={loss:.4f}, Precisión={acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0d2bc646-a7bb-4c48-9c79-99ea6a48262d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Época 0: Pérdida=2.3244, Precisión=0.1737\n",
      "Época 10: Pérdida=1.8287, Precisión=0.1873\n",
      "Época 20: Pérdida=1.8014, Precisión=0.1960\n",
      "Época 30: Pérdida=1.7897, Precisión=0.1948\n",
      "Época 40: Pérdida=1.7828, Precisión=0.2035\n",
      "Época 50: Pérdida=1.7779, Precisión=0.2134\n",
      "Época 60: Pérdida=1.7740, Precisión=0.2171\n",
      "Época 70: Pérdida=1.7706, Precisión=0.2146\n",
      "Época 80: Pérdida=1.7676, Precisión=0.2109\n",
      "Época 90: Pérdida=1.7650, Precisión=0.2196\n",
      "Época 100: Pérdida=1.7624, Precisión=0.2246\n",
      "Época 110: Pérdida=1.7600, Precisión=0.2258\n",
      "Época 120: Pérdida=1.7577, Precisión=0.2246\n",
      "Época 130: Pérdida=1.7557, Precisión=0.2320\n",
      "Época 140: Pérdida=1.7537, Precisión=0.2370\n",
      "Época 150: Pérdida=1.7519, Precisión=0.2407\n",
      "Época 160: Pérdida=1.7499, Precisión=0.2419\n",
      "Época 170: Pérdida=1.7482, Precisión=0.2481\n",
      "Época 180: Pérdida=1.7468, Precisión=0.2494\n",
      "Época 190: Pérdida=1.7455, Precisión=0.2519\n",
      "Precisión final en test: 0.1634\n"
     ]
    }
   ],
   "source": [
    "mlp = MLP_Flexible(capas=[4, 32, 16, len(np.unique(y))], lr=0.1)\n",
    "mlp.train(X_train, y_train, epochs)\n",
    "y_pred = mlp.forward(X_test)\n",
    "acc = np.mean(np.argmax(y_pred, axis=1) == y_test)\n",
    "print(f\"Precisión final en test: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad13fd2-8f70-49ba-b97a-4190ff2e4995",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
